{"cells":[{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":603,"status":"ok","timestamp":1734546035315,"user":{"displayName":"Manoj Kumar Baniya","userId":"17588784896461812175"},"user_tz":-345},"id":"MiV8fs990H92"},"outputs":[],"source":["roman_stoi = {'?': 0,\n"," '_': 1,\n"," '\u003c': 2,\n"," '\u003e': 3,\n"," 'a': 4,\n"," 'b': 5,\n"," 'c': 6,\n"," 'd': 7,\n"," 'e': 8,\n"," 'f': 9,\n"," 'g': 10,\n"," 'h': 11,\n"," 'i': 12,\n"," 'j': 13,\n"," 'k': 14,\n"," 'l': 15,\n"," 'm': 16,\n"," 'n': 17,\n"," 'o': 18,\n"," 'p': 19,\n"," 'q': 20,\n"," 'r': 21,\n"," 's': 22,\n"," 't': 23,\n"," 'u': 24,\n"," 'v': 25,\n"," 'w': 26,\n"," 'x': 27,\n"," 'y': 28,\n"," 'z': 29}\n","\n","devnagari_stoi = {'?': 0,\n"," '_': 1,\n"," 'ँ': 2,\n"," 'ं': 3,\n"," 'ः': 4,\n"," 'अ': 5,\n"," 'आ': 6,\n"," 'इ': 7,\n"," 'ई': 8,\n"," 'उ': 9,\n"," 'ऊ': 10,\n"," 'ऋ': 11,\n"," 'ए': 12,\n"," 'ऐ': 13,\n"," 'ऑ': 14,\n"," 'ओ': 15,\n"," 'औ': 16,\n"," 'क': 17,\n"," 'ख': 18,\n"," 'ग': 19,\n"," 'घ': 20,\n"," 'ङ': 21,\n"," 'च': 22,\n"," 'छ': 23,\n"," 'ज': 24,\n"," 'झ': 25,\n"," 'ञ': 26,\n"," 'ट': 27,\n"," 'ठ': 28,\n"," 'ड': 29,\n"," 'ढ': 30,\n"," 'ण': 31,\n"," 'त': 32,\n"," 'थ': 33,\n"," 'द': 34,\n"," 'ध': 35,\n"," 'न': 36,\n"," 'प': 37,\n"," 'फ': 38,\n"," 'ब': 39,\n"," 'भ': 40,\n"," 'म': 41,\n"," 'य': 42,\n"," 'र': 43,\n"," 'ल': 44,\n"," 'व': 45,\n"," 'श': 46,\n"," 'ष': 47,\n"," 'स': 48,\n"," 'ह': 49,\n"," '़': 50,\n"," 'ऽ': 51,\n"," 'ा': 52,\n"," 'ि': 53,\n"," 'ी': 54,\n"," 'ु': 55,\n"," 'ू': 56,\n"," 'ृ': 57,\n"," 'े': 58,\n"," 'ै': 59,\n"," 'ॉ': 60,\n"," 'ो': 61,\n"," 'ौ': 62,\n"," '्': 63,\n"," '॰': 64}\n","\n","roman_itos = {v: k for k, v in roman_stoi.items()}\n","devnagari_itos = {v: k for k, v in devnagari_stoi.items()}\n","\n","unk_token = \"?\"\n","pad_token = \"_\"\n","start_token = \"\u003c\"\n","end_token = \"\u003e\"\n","\n","special_tokens = [unk_token, pad_token, start_token, end_token]"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1734546035930,"user":{"displayName":"Manoj Kumar Baniya","userId":"17588784896461812175"},"user_tz":-345},"id":"CPMoYM_o0US4","outputId":"8e54c218-63bc-4f52-b54c-ee2419d43f9d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[[1. 0. 0. ... 0. 0. 0.]\n","  [0. 0. 1. ... 0. 0. 0.]\n","  [0. 1. 0. ... 0. 0. 0.]\n","  ...\n","  [0. 1. 0. ... 0. 0. 0.]\n","  [0. 1. 0. ... 0. 0. 0.]\n","  [0. 1. 0. ... 0. 0. 0.]]]\n"]}],"source":["import re\n","import numpy as np\n","\n","nepali_to_english_numbers = {\n","    \"०\": \"0\",\n","    \"१\": \"1\",\n","    \"२\": \"2\",\n","    \"३\": \"3\",\n","    \"४\": \"4\",\n","    \"५\": \"5\",\n","    \"६\": \"6\",\n","    \"७\": \"7\",\n","    \"८\": \"8\",\n","    \"९\": \"9\",\n","    \"।\": \".\"\n","}\n","\n","def clean_and_map_nepali_text(text):\n","    \"\"\"clean the devnagari text\n","    Args:\n","        text (str): Input Nepali text.\n","\n","    Returns:\n","        str: Cleaned and processed text.\n","    \"\"\"\n","    # Define the characters to remove (punctuation and special symbols)\n","    pattern = r\"[,!?\\\"'।।‘’“”():;—-]\"\n","\n","    # Remove punctuation\n","    cleaned_text = re.sub(pattern, \"\", text)\n","\n","    return cleaned_text\n","\n","def one_hot_encode_tokens(tokens: list):\n","    \"\"\"One hot encode the token list\"\"\"\n","    MAX_ENCODER_SEQUENCE_LENGTH = 65\n","    MAX_ENCODER_TOKENS = 19\n","    ohe = np.zeros((MAX_ENCODER_TOKENS, MAX_ENCODER_SEQUENCE_LENGTH))\n","    for t, tkn in enumerate(tokens):\n","        ohe[t, tkn] = 1.0\n","    # for other just encode the position of the pad token\n","\n","    t = len(tokens) - 1 if tokens else -1\n","    ohe[t+1:, devnagari_stoi[pad_token]] = 1.0\n","    ohe = ohe.reshape(1, ohe.shape[0], ohe.shape[1])\n","    return ohe\n","\n","if __name__ == '__main__':\n","    # Example usage\n","       # Example usage\n","    nepali_text = \"०७८ साउन १९ मा एकैदिन चार करोड ९१ लाख ९४ हजार रुपैयाँ सारिएको प्रतिवेदनमा उल्लेख छ । गोर्खा मिडियाका उपाध्यक्ष छविलाल जोशीको घरबाट बरामद भएको हार्ड डिस्कमा सानो पाइला सहकारीबाट पनि रकम सिधै गोर्खा मिडियामा सारिएको फेला परेको हो । प्रहरीका अनुसार सानो पाइला सहकारीका अध्यक्ष अनन्तबाबु राई, सचिव देवेन्द्रबाबु राई, कोषाध्यक्ष कुमार रम्तेल, प्रबन्धक असरफ अली सिद्धिकी, संरक्षक गीतेन्द्रबाबु (जीबी) राईलगायत ११ जना हालसम्म फरार छन् । सहकारीकी पूर्वउपाध्यक्ष नेहा पौडेल गत साउन २१ मा जिल्ला अदालत पर्साबाट न्यायीक परीक्षण हुँदा ठहरेबमोजिम हुने गरी हाललाई एक करोड १० लाख धरौटीमा रिहा भइन् । यसैगरी सहकारीका कर्मचारी राधेचन्द्र यादव पनि ३५ लाख रुपैयाँ धरौटीमा रिहा भएका छन् ।\"\n","    cleaned_text = clean_and_map_nepali_text(nepali_text)\n","\n","    # print(cleaned_text)\n","    tokens = [0,2]\n","    ohe = one_hot_encode_tokens(tokens)\n","    print(ohe)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1734546035930,"user":{"displayName":"Manoj Kumar Baniya","userId":"17588784896461812175"},"user_tz":-345},"id":"4qp8rnC10Z3U","outputId":"7fd0fb69-931b-4442-b2d7-3c11f184318e"},"outputs":[{"name":"stdout","output_type":"stream","text":["_____ORIGINAL_____\n","Text: सहकारी hello र pसम्पत033 k3j43\n","\n","_____DEVANAGARI_TOKENS_____\n","[[48, 49, 17, 52, 43, 54], [0, 0, 0, 0, 0], [43], [0, 48, 41, 63, 37, 32, 0, 0, 0], [0, 0, 0, 0, 0]]\n","\n","_____ROMAN_TOKENS_____\n","[[0, 0, 0, 0, 0, 0], [11, 8, 15, 15, 18], [0], [19, 0, 0, 0, 0, 0, 0, 0, 0], [14, 0, 13, 0, 0]]\n","\n","_____DETOKENIZED_DEVANAGARI_TOKENS_____\n","['सहकारी', '?????', 'र', '?सम्पत???', '?????']\n","_____DETOKENIZED_ROMAN_TOKENS_____\n","\n","['??????', 'hello', '?', 'p????????', 'k?j??']\n"]}],"source":["\n","\n","class Tokenizer:\n","    def __init__(self):\n","        self.pad_token = roman_stoi[special_tokens[1]]\n","        self.start_token = roman_stoi[special_tokens[2]]\n","        self.end_token = roman_stoi[special_tokens[3]]\n","        self.unk_token = roman_stoi[special_tokens[0]]\n","\n","    def tokenize_devnagari(self, text: list[str]):\n","        \"\"\"tokenize the devnagari text into token\n","\n","        args:\n","            text: str: The list of devnagari text to be tokenized\n","\n","        returns:\n","            list: The list of tokens\n","        \"\"\"\n","        tokens = []\n","        for word in text:\n","            token = []\n","            for character in word:\n","                try:\n","                    token.append(devnagari_stoi[character])\n","                except KeyError:\n","                    token.append(self.unk_token)\n","            tokens.append(token)\n","        return tokens\n","\n","    def detokenize_devnagari(self, tokens: list[list[int]]):\n","        \"\"\"detokenize the roman tokens into text\n","\n","        args:\n","            tokens: list: The list of tokens to be detokenized\n","\n","        returns:\n","            str: The detokenized list of text\n","        \"\"\"\n","        texts = []\n","        for token in tokens:\n","            text = \"\".join([devnagari_itos[character] for character in token])\n","            texts.append(text)\n","        return texts\n","\n","    def tokenize_roman(self, text: list[str]):\n","        \"\"\"tokenize the roman text into token\n","\n","        args:\n","            text: str: The roman text to be tokenized\n","\n","        returns:\n","            list: The list of tokens\n","        \"\"\"\n","        tokens = []\n","        for word in text:\n","            token = []\n","            for character in word:\n","                try:\n","                    token.append(roman_stoi[character])\n","                except KeyError:\n","                    token.append(self.unk_token)\n","            tokens.append(token)\n","        return tokens\n","\n","    def detokenize_roman(self, tokens: list[list[int]]):\n","        \"\"\"detokenize the roman tokens into text\n","\n","        args:\n","            tokens: list: The list of tokens to be detokenized\n","\n","        returns:\n","            str: The detokenized list of text\n","        \"\"\"\n","        texts = []\n","        for tkn in tokens:\n","            text = \"\".join([roman_itos[token] for token in tkn])\n","            texts.append(text)\n","        return texts\n","\n","if __name__ == '__main__':\n","    tokenizer = Tokenizer()\n","    print(\"_____ORIGINAL_____\")\n","    text = \"सहकारी hello र pसम्पत033 k3j43\"\n","    print(f\"Text: {text}\\n\")\n","\n","    devnagari_tokens = tokenizer.tokenize_devnagari(text.split())\n","    devnagari_text = tokenizer.detokenize_devnagari(devnagari_tokens)\n","    print(\"_____DEVANAGARI_TOKENS_____\")\n","    print(f\"{devnagari_tokens}\\n\")\n","\n","    roman_tokens = tokenizer.tokenize_roman(text.split())\n","    roman_text = tokenizer.detokenize_roman(roman_tokens)\n","    print(\"_____ROMAN_TOKENS_____\")\n","    print(roman_tokens)\n","\n","    unk_ = tokenizer.detokenize_devnagari(devnagari_tokens)\n","    print(\"\\n_____DETOKENIZED_DEVANAGARI_TOKENS_____\")\n","    print(f\"{unk_}\")\n","\n","    unk_ = tokenizer.detokenize_roman(roman_tokens)\n","    print(\"_____DETOKENIZED_ROMAN_TOKENS_____\")\n","    print(f\"\\n{unk_}\")"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1734546035930,"user":{"displayName":"Manoj Kumar Baniya","userId":"17588784896461812175"},"user_tz":-345},"id":"BIdDNVgn09Ng"},"outputs":[],"source":["def map_numbers(text):\n","    # Replace Nepali numbers with English numbers\n","    for nep_num, eng_num in nepali_to_english_numbers.items():\n","        text = text.replace(nep_num, eng_num)\n","\n","    return text\n","\n","def clean(text):\n","    # clean the text\n","    cleaned_text = clean_and_map_nepali_text(text)\n","    # make a list of text\n","    tokenized_text = cleaned_text.split(\" \")\n","\n","    return tokenized_text"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1734546035930,"user":{"displayName":"Manoj Kumar Baniya","userId":"17588784896461812175"},"user_tz":-345},"id":"wKOnGGOu0dko"},"outputs":[],"source":["import keras\n","import numpy as np\n","import tensorflow as tf\n","from functools import lru_cache\n","\n","class Model:\n","    _instance = None  # Singleton pattern for caching model\n","\n","    def __new__(cls, model_path):\n","        if not cls._instance:\n","            cls._instance = super().__new__(cls)\n","        return cls._instance\n","\n","    def __init__(self, model_path):\n","        if not hasattr(self, 'model'):\n","            self.model = keras.models.load_model(model_path)\n","            self.LATENT_DIM = 256\n","            # Warm up the models\n","            self._encoder_model = self._create_encoder()\n","            self._decoder_model = self._create_decoder()\n","\n","    def _create_encoder(self):\n","        \"\"\"Create encoder model only once\"\"\"\n","        encoder_inputs = self.model.input[0]\n","        encoder_outputs, state_h_enc, state_c_enc = self.model.layers[2].output\n","        return keras.Model(encoder_inputs, [state_h_enc, state_c_enc])\n","\n","    def _create_decoder(self):\n","        \"\"\"Create decoder model only once\"\"\"\n","        decoder_inputs = self.model.input[1]\n","        decoder_state_input_h = keras.Input(shape=(self.LATENT_DIM,), name=\"decoder_state_1\")\n","        decoder_state_input_c = keras.Input(shape=(self.LATENT_DIM,), name=\"decoder_state_2\")\n","        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","\n","        decoder_lstm = self.model.layers[3]\n","        decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n","            decoder_inputs, initial_state=decoder_states_inputs)\n","\n","        decoder_dense = self.model.layers[4]\n","        decoder_outputs = decoder_dense(decoder_outputs)\n","\n","        return keras.Model(\n","            [decoder_inputs] + decoder_states_inputs,\n","            [decoder_outputs, state_h_dec, state_c_dec]\n","        )\n","\n","    @property\n","    def encoder_model(self):\n","        return self._encoder_model\n","\n","    @property\n","    def decoder_model(self):\n","        return self._decoder_model"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":710,"status":"ok","timestamp":1734546036638,"user":{"displayName":"Manoj Kumar Baniya","userId":"17588784896461812175"},"user_tz":-345},"id":"14PDassg0ldj"},"outputs":[],"source":["MODEL_LOCATION=\"./model_1.keras\"\n","NUM_DECODER_TOKENS=30\n","MAX_OUTPUT_SEQUENCE_LENGTH=100"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1734546036638,"user":{"displayName":"Manoj Kumar Baniya","userId":"17588784896461812175"},"user_tz":-345},"id":"8EO5dXh40hkS"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from functools import wraps, lru_cache\n","\n","def array_to_tuple(arr):\n","    \"\"\"Convert NumPy array to a hashable tuple representation\"\"\"\n","    if isinstance(arr, np.ndarray):\n","        # For 2D or 3D arrays, convert to nested tuples\n","        if arr.ndim == 3:\n","            return tuple(tuple(tuple(row) for row in layer) for layer in arr)\n","        elif arr.ndim == 2:\n","            return tuple(tuple(row) for row in arr)\n","        elif arr.ndim == 1:\n","            return tuple(arr)\n","    return arr\n","\n","def tuple_to_array(tup):\n","    \"\"\"Convert tuple representation back to NumPy array\"\"\"\n","    if isinstance(tup, tuple):\n","        # Handle 3D array case\n","        if tup and isinstance(tup[0], tuple) and isinstance(tup[0][0], tuple):\n","            return np.array([list(list(row) for row in layer) for layer in tup])\n","        # Handle 2D array case\n","        elif tup and isinstance(tup[0], tuple):\n","            return np.array(list(list(row) for row in tup))\n","        # Handle 1D array case\n","        else:\n","            return np.array(tup)\n","    return tup\n","\n","def cached_decode_sequence(maxsize=1000):\n","    \"\"\"Custom caching decorator that handles NumPy arrays\"\"\"\n","    def decorator(func):\n","        cache = {}\n","\n","        @wraps(func)\n","        def wrapper(input_seq, *args, **kwargs):\n","            # Convert input to hashable form\n","            hashable_key = array_to_tuple(input_seq)\n","\n","            # Check if result is already in cache\n","            if hashable_key in cache:\n","                return cache[hashable_key]\n","\n","            # Compute result\n","            result = func(input_seq, *args, **kwargs)\n","\n","            # Store in cache\n","            cache[hashable_key] = result\n","\n","            # Limit cache size\n","            if len(cache) \u003e maxsize:\n","                cache.popitem()\n","\n","            return result\n","\n","        return wrapper\n","    return decorator\n","\n","@cached_decode_sequence(maxsize=1000)\n","def decode_sequence(input_seq, model_path=MODEL_LOCATION):\n","    \"\"\"Optimized sequence decoding with custom caching\"\"\"\n","    # Ensure input_seq is a NumPy array\n","    if isinstance(input_seq, tuple):\n","        input_seq = tuple_to_array(input_seq)\n","\n","    model = Model(model_path)\n","\n","    # Convert input to tensor for potential performance boost\n","    input_seq = tf.convert_to_tensor(input_seq, dtype=tf.float32)\n","\n","    # Encode the input as state vectors.\n","    states_value = model.encoder_model.predict(input_seq, verbose=0)\n","    states_value = [tf.convert_to_tensor(state) for state in states_value]\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1, NUM_DECODER_TOKENS), dtype=np.float32)\n","    target_seq[0, 0, roman_stoi[\"\u003c\"]] = 1.0\n","\n","    # Sampling loop with early stopping\n","    decoded_sentence = \"\"\n","    for _ in range(MAX_OUTPUT_SEQUENCE_LENGTH):\n","        output_tokens, h, c = model.decoder_model.predict(\n","            [target_seq] + states_value, verbose=0\n","        )\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = roman_itos[sampled_token_index]\n","\n","        # Exit conditions\n","        if sampled_char == \"\u003e\" or len(decoded_sentence) \u003e= MAX_OUTPUT_SEQUENCE_LENGTH:\n","            break\n","\n","        decoded_sentence += sampled_char\n","\n","        # Update the target sequence\n","        target_seq = np.zeros((1, 1, NUM_DECODER_TOKENS), dtype=np.float32)\n","        target_seq[0, 0, sampled_token_index] = 1.0\n","\n","        # Update states\n","        states_value = [h, c]\n","\n","    return decoded_sentence"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1734546036638,"user":{"displayName":"Manoj Kumar Baniya","userId":"17588784896461812175"},"user_tz":-345},"id":"FPzBooc-0vXW"},"outputs":[],"source":["def transliterate(text: str) -\u003e str:\n","    # Tokenization and one-hot encoding in one step\n","    tokenized_devnagari = tokenizer.tokenize_devnagari(text)\n","\n","    # Ensure non-empty tokenization\n","    if not tokenized_devnagari:\n","        return \"\"\n","\n","    # Convert in number with error handling\n","    try:\n","        ohe_token = one_hot_encode_tokens(tokenized_devnagari[0])\n","    except Exception as e:\n","        print(f\"Encoding error for text '{text}': {e}\")\n","        return \"\"\n","\n","    # Translation with caching\n","    return decode_sequence(ohe_token)"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1734546036638,"user":{"displayName":"Manoj Kumar Baniya","userId":"17588784896461812175"},"user_tz":-345},"id":"BhGfHrp70xGX"},"outputs":[],"source":["def process_text_batch(text, batch_size=128):\n","    \"\"\"Process text in batches for potential performance improvement\"\"\"\n","    sentences = text.split(\"।\")\n","    roman_translation = []\n","\n","    for i in range(0, len(sentences), batch_size):\n","        batch = sentences[i:i+batch_size]\n","        batch_translations = []\n","\n","        for line in batch:\n","            words = line.split()\n","            translated_sentence = []\n","\n","            for word in words:\n","                if any(char.isdigit() for char in word):\n","                    translated_sentence.append(map_numbers(word))\n","                else:\n","                    clean_text = clean(word)\n","                    if clean_text:\n","                        translated_sentence.append(transliterate(clean_text))\n","\n","            # Post-processing\n","            translated_sentence = \" \".join(translated_sentence).strip()\n","            if translated_sentence:\n","                translated_sentence = translated_sentence[0].upper() + translated_sentence[1:] + \".\"\n","                batch_translations.append(translated_sentence)\n","\n","        roman_translation.extend(batch_translations)\n","\n","    return roman_translation"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7758,"status":"ok","timestamp":1734546044395,"user":{"displayName":"Manoj Kumar Baniya","userId":"17588784896461812175"},"user_tz":-345},"id":"w80bJ-BD1cLY","outputId":"6fd76d17-44d7-4398-c3bd-50c813d87b9f"},"outputs":[{"data":{"text/plain":["['Tasbirharu aigensi chiniyan sahar juhaiko hundai makau jodne.']"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["process_text_batch(\"तस्बिरहरु ऐजेन्सी चिनियाँ सहर जुहाईको हुँदै मकाउ जोड्ने\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"7Vq9c0Ez23nP"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing: 0.00% (1/36638)\n","Processing: 0.01% (2/36638)\n","Processing: 0.01% (3/36638)\n","Processing: 0.01% (4/36638)\n","Processing: 0.01% (5/36638)\n","Processing: 0.02% (6/36638)\n","Processing: 0.02% (7/36638)\n","Processing: 0.02% (8/36638)\n","Processing: 0.02% (9/36638)\n","Processing: 0.03% (10/36638)\n","Processing: 0.03% (11/36638)\n","Processing: 0.03% (12/36638)\n","Processing: 0.04% (13/36638)\n","Processing: 0.04% (14/36638)\n","Processing: 0.04% (15/36638)\n","Processing: 0.04% (16/36638)\n","Processing: 0.05% (17/36638)\n","Processing: 0.05% (18/36638)\n","Processing: 0.05% (19/36638)\n","Processing: 0.05% (20/36638)\n","Processing: 0.06% (21/36638)\n","Processing: 0.06% (22/36638)\n","Processing: 0.06% (23/36638)\n","Processing: 0.07% (24/36638)\n","Processing: 0.07% (25/36638)\n","Processing: 0.07% (26/36638)\n","Processing: 0.07% (27/36638)\n","Processing: 0.08% (28/36638)\n","Processing: 0.08% (29/36638)\n","Processing: 0.08% (30/36638)\n","Processing: 0.08% (31/36638)\n","Processing: 0.09% (32/36638)\n","Processing: 0.09% (33/36638)\n","Processing: 0.09% (34/36638)\n","Processing: 0.10% (35/36638)\n","Processing: 0.10% (36/36638)\n","Processing: 0.10% (37/36638)\n","Processing: 0.10% (38/36638)\n","Processing: 0.11% (39/36638)\n","Processing: 0.11% (40/36638)\n","Processing: 0.11% (41/36638)\n","Processing: 0.11% (42/36638)\n","Processing: 0.12% (43/36638)\n","Processing: 0.12% (44/36638)\n","Processing: 0.12% (45/36638)\n","Processing: 0.13% (46/36638)\n","Processing: 0.13% (47/36638)\n","Processing: 0.13% (48/36638)\n","Processing: 0.13% (49/36638)\n","Processing: 0.14% (50/36638)\n","Processing: 0.14% (51/36638)\n","Processing: 0.14% (52/36638)\n","Processing: 0.14% (53/36638)\n","Processing: 0.15% (54/36638)\n","Processing: 0.15% (55/36638)\n","Processing: 0.15% (56/36638)\n","Processing: 0.16% (57/36638)\n","Processing: 0.16% (58/36638)\n","Processing: 0.16% (59/36638)\n","Processing: 0.16% (60/36638)\n","Processing: 0.17% (61/36638)\n","Processing: 0.17% (62/36638)\n","Processing: 0.17% (63/36638)\n","Processing: 0.17% (64/36638)\n","Processing: 0.18% (65/36638)\n","Processing: 0.18% (66/36638)\n","Processing: 0.18% (67/36638)\n","Processing: 0.19% (68/36638)\n","Processing: 0.19% (69/36638)\n","Processing: 0.19% (70/36638)\n","Processing: 0.19% (71/36638)\n","Processing: 0.20% (72/36638)\n","Processing: 0.20% (73/36638)\n","Processing: 0.20% (74/36638)\n","Processing: 0.20% (75/36638)\n","Processing: 0.21% (76/36638)\n","Processing: 0.21% (77/36638)\n","Processing: 0.21% (78/36638)\n","Processing: 0.22% (79/36638)\n","Processing: 0.22% (80/36638)\n","Processing: 0.22% (81/36638)\n","Processing: 0.22% (82/36638)\n","Processing: 0.23% (83/36638)\n","Processing: 0.23% (84/36638)\n","Processing: 0.23% (85/36638)\n","Processing: 0.23% (86/36638)\n","Processing: 0.24% (87/36638)\n","Processing: 0.24% (88/36638)\n","Processing: 0.24% (89/36638)\n","Processing: 0.25% (90/36638)\n","Processing: 0.25% (91/36638)\n","Processing: 0.25% (92/36638)\n","Processing: 0.25% (93/36638)\n","Processing: 0.26% (94/36638)\n","Processing: 0.26% (95/36638)\n","Processing: 0.26% (96/36638)\n","Processing: 0.26% (97/36638)\n","Processing: 0.27% (98/36638)\n","Processing: 0.27% (99/36638)\n","Processing: 0.27% (100/36638)\n","Processing: 0.28% (101/36638)\n","Processing: 0.28% (102/36638)\n","Processing: 0.28% (103/36638)\n","Processing: 0.28% (104/36638)\n","Processing: 0.29% (105/36638)\n","Processing: 0.29% (106/36638)\n","Processing: 0.29% (107/36638)\n","Processing: 0.29% (108/36638)\n","Processing: 0.30% (109/36638)\n","Processing: 0.30% (110/36638)\n","Processing: 0.30% (111/36638)\n","Processing: 0.31% (112/36638)\n","Processing: 0.31% (113/36638)\n","Processing: 0.31% (114/36638)\n","Processing: 0.31% (115/36638)\n","Processing: 0.32% (116/36638)\n","Processing: 0.32% (117/36638)\n","Processing: 0.32% (118/36638)\n","Processing: 0.32% (119/36638)\n","Processing: 0.33% (120/36638)\n","Processing: 0.33% (121/36638)\n","Processing: 0.33% (122/36638)\n","Processing: 0.34% (123/36638)\n","Processing: 0.34% (124/36638)\n","Processing: 0.34% (125/36638)\n","Processing: 0.34% (126/36638)\n","Processing: 0.35% (127/36638)\n","Processing: 0.35% (128/36638)\n","Processing: 0.35% (129/36638)\n","Processing: 0.35% (130/36638)\n","Processing: 0.36% (131/36638)\n","Processing: 0.36% (132/36638)\n","Processing: 0.36% (133/36638)\n","Processing: 0.37% (134/36638)\n","Processing: 0.37% (135/36638)\n","Processing: 0.37% (136/36638)\n","Processing: 0.37% (137/36638)\n","Processing: 0.38% (138/36638)\n","Processing: 0.38% (139/36638)\n","Processing: 0.38% (140/36638)\n","Processing: 0.38% (141/36638)\n","Processing: 0.39% (142/36638)\n","Processing: 0.39% (143/36638)\n","Processing: 0.39% (144/36638)\n","Processing: 0.40% (145/36638)\n","Processing: 0.40% (146/36638)\n","Processing: 0.40% (147/36638)\n","Processing: 0.40% (148/36638)\n","Processing: 0.41% (149/36638)\n","Processing: 0.41% (150/36638)\n","Processing: 0.41% (151/36638)\n","Processing: 0.41% (152/36638)\n","Processing: 0.42% (153/36638)\n","Processing: 0.42% (154/36638)\n","Processing: 0.42% (155/36638)\n","Processing: 0.43% (156/36638)\n","Processing: 0.43% (157/36638)\n","Processing: 0.43% (158/36638)\n","Processing: 0.43% (159/36638)\n","Processing: 0.44% (160/36638)\n","Processing: 0.44% (161/36638)\n","Processing: 0.44% (162/36638)\n","Processing: 0.44% (163/36638)\n","Processing: 0.45% (164/36638)\n","Processing: 0.45% (165/36638)\n","Processing: 0.45% (166/36638)\n","Processing: 0.46% (167/36638)\n","Processing: 0.46% (168/36638)\n","Processing: 0.46% (169/36638)\n","Processing: 0.46% (170/36638)\n","Processing: 0.47% (171/36638)\n","Processing: 0.47% (172/36638)\n","Processing: 0.47% (173/36638)\n","Processing: 0.47% (174/36638)\n","Processing: 0.48% (175/36638)\n","Processing: 0.48% (176/36638)\n","Processing: 0.48% (177/36638)\n","Processing: 0.49% (178/36638)\n","Processing: 0.49% (179/36638)\n","Processing: 0.49% (180/36638)\n","Processing: 0.49% (181/36638)\n","Processing: 0.50% (182/36638)\n","Processing: 0.50% (183/36638)\n","Processing: 0.50% (184/36638)\n","Processing: 0.50% (185/36638)\n","Processing: 0.51% (186/36638)\n","Processing: 0.51% (187/36638)\n","Processing: 0.51% (188/36638)\n","Processing: 0.52% (189/36638)\n","Processing: 0.52% (190/36638)\n","Processing: 0.52% (191/36638)\n","Processing: 0.52% (192/36638)\n","Processing: 0.53% (193/36638)\n","Processing: 0.53% (194/36638)\n","Processing: 0.53% (195/36638)\n","Processing: 0.53% (196/36638)\n","Processing: 0.54% (197/36638)\n","Processing: 0.54% (198/36638)\n","Processing: 0.54% (199/36638)\n","Processing: 0.55% (200/36638)\n","Processing: 0.55% (201/36638)\n","Processing: 0.55% (202/36638)\n","Processing: 0.55% (203/36638)\n","Processing: 0.56% (204/36638)\n","Processing: 0.56% (205/36638)\n","Processing: 0.56% (206/36638)\n","Processing: 0.56% (207/36638)\n","Processing: 0.57% (208/36638)\n","Processing: 0.57% (209/36638)\n","Processing: 0.57% (210/36638)\n","Processing: 0.58% (211/36638)\n","Processing: 0.58% (212/36638)\n","Processing: 0.58% (213/36638)\n","Processing: 0.58% (214/36638)\n","Processing: 0.59% (215/36638)\n","Processing: 0.59% (216/36638)\n","Processing: 0.59% (217/36638)\n","Processing: 0.60% (218/36638)\n","Processing: 0.60% (219/36638)\n","Processing: 0.60% (220/36638)\n","Processing: 0.60% (221/36638)\n","Processing: 0.61% (222/36638)\n","Processing: 0.61% (223/36638)\n","Processing: 0.61% (224/36638)\n","Processing: 0.61% (225/36638)\n","Processing: 0.62% (226/36638)\n","Processing: 0.62% (227/36638)\n","Processing: 0.62% (228/36638)\n","Processing: 0.63% (229/36638)\n","Processing: 0.63% (230/36638)\n","Processing: 0.63% (231/36638)\n","Processing: 0.63% (232/36638)\n","Processing: 0.64% (233/36638)\n","Processing: 0.64% (234/36638)\n","Processing: 0.64% (235/36638)\n","Processing: 0.64% (236/36638)\n","Encoding error for text '['एट्रिब्युसन/सेयरअलाइक']': index 19 is out of bounds for axis 0 with size 19\n","Processing: 0.65% (237/36638)\n","Processing: 0.65% (238/36638)\n","Processing: 0.65% (239/36638)\n","Processing: 0.66% (240/36638)\n","Processing: 0.66% (241/36638)\n","Processing: 0.66% (242/36638)\n","Processing: 0.66% (243/36638)\n","Processing: 0.67% (244/36638)\n","Processing: 0.67% (245/36638)\n","Processing: 0.67% (246/36638)\n","Processing: 0.67% (247/36638)\n","Processing: 0.68% (248/36638)\n","Processing: 0.68% (249/36638)\n","Processing: 0.68% (250/36638)\n","Processing: 0.69% (251/36638)\n","Processing: 0.69% (252/36638)\n","Processing: 0.69% (253/36638)\n","Processing: 0.69% (254/36638)\n","Processing: 0.70% (255/36638)\n","Processing: 0.70% (256/36638)\n","Processing: 0.70% (257/36638)\n","Processing: 0.70% (258/36638)\n","Processing: 0.71% (259/36638)\n","Processing: 0.71% (260/36638)\n","Processing: 0.71% (261/36638)\n","Processing: 0.72% (262/36638)\n","Processing: 0.72% (263/36638)\n","Processing: 0.72% (264/36638)\n","Processing: 0.72% (265/36638)\n","Processing: 0.73% (266/36638)\n","Processing: 0.73% (267/36638)\n","Processing: 0.73% (268/36638)\n","Processing: 0.73% (269/36638)\n","Processing: 0.74% (270/36638)\n","Processing: 0.74% (271/36638)\n","Processing: 0.74% (272/36638)\n","Processing: 0.75% (273/36638)\n","Processing: 0.75% (274/36638)\n","Processing: 0.75% (275/36638)\n","Processing: 0.75% (276/36638)\n","Processing: 0.76% (277/36638)\n","Processing: 0.76% (278/36638)\n","Processing: 0.76% (279/36638)\n","Processing: 0.76% (280/36638)\n","Processing: 0.77% (281/36638)\n","Processing: 0.77% (282/36638)\n","Processing: 0.77% (283/36638)\n","Processing: 0.78% (284/36638)\n","Processing: 0.78% (285/36638)\n","Processing: 0.78% (286/36638)\n","Processing: 0.78% (287/36638)\n","Processing: 0.79% (288/36638)\n","Processing: 0.79% (289/36638)\n","Processing: 0.79% (290/36638)\n","Processing: 0.79% (291/36638)\n","Processing: 0.80% (292/36638)\n","Processing: 0.80% (293/36638)\n","Processing: 0.80% (294/36638)\n","Processing: 0.81% (295/36638)\n","Processing: 0.81% (296/36638)\n","Processing: 0.81% (297/36638)\n","Processing: 0.81% (298/36638)\n","Processing: 0.82% (299/36638)\n","Processing: 0.82% (300/36638)\n","Processing: 0.82% (301/36638)\n","Processing: 0.82% (302/36638)\n","Processing: 0.83% (303/36638)\n","Processing: 0.83% (304/36638)\n","Processing: 0.83% (305/36638)\n","Processing: 0.84% (306/36638)\n","Processing: 0.84% (307/36638)\n","Processing: 0.84% (308/36638)\n","Processing: 0.84% (309/36638)\n","Processing: 0.85% (310/36638)\n","Processing: 0.85% (311/36638)\n","Processing: 0.85% (312/36638)\n","Processing: 0.85% (313/36638)\n","Processing: 0.86% (314/36638)\n","Processing: 0.86% (315/36638)\n","Processing: 0.86% (316/36638)\n","Processing: 0.87% (317/36638)\n","Processing: 0.87% (318/36638)\n","Processing: 0.87% (319/36638)\n","Processing: 0.87% (320/36638)\n","Processing: 0.88% (321/36638)\n","Processing: 0.88% (322/36638)\n","Processing: 0.88% (323/36638)\n","Processing: 0.88% (324/36638)\n","Processing: 0.89% (325/36638)\n","Processing: 0.89% (326/36638)\n","Processing: 0.89% (327/36638)\n","Processing: 0.90% (328/36638)\n","Processing: 0.90% (329/36638)\n","Processing: 0.90% (330/36638)\n","Processing: 0.90% (331/36638)\n","Processing: 0.91% (332/36638)\n","Processing: 0.91% (333/36638)\n","Processing: 0.91% (334/36638)\n","Processing: 0.91% (335/36638)\n","Processing: 0.92% (336/36638)\n","Processing: 0.92% (337/36638)\n","Processing: 0.92% (338/36638)\n","Processing: 0.93% (339/36638)\n","Processing: 0.93% (340/36638)\n","Processing: 0.93% (341/36638)\n","Processing: 0.93% (342/36638)\n","Processing: 0.94% (343/36638)\n","Processing: 0.94% (344/36638)\n","Processing: 0.94% (345/36638)\n","Processing: 0.94% (346/36638)\n","Processing: 0.95% (347/36638)\n","Processing: 0.95% (348/36638)\n","Processing: 0.95% (349/36638)\n","Processing: 0.96% (350/36638)\n","Processing: 0.96% (351/36638)\n","Processing: 0.96% (352/36638)\n","Processing: 0.96% (353/36638)\n","Processing: 0.97% (354/36638)\n","Processing: 0.97% (355/36638)\n","Processing: 0.97% (356/36638)\n","Processing: 0.97% (357/36638)\n","Processing: 0.98% (358/36638)\n","Processing: 0.98% (359/36638)\n","Processing: 0.98% (360/36638)\n","Processing: 0.99% (361/36638)\n","Processing: 0.99% (362/36638)\n","Processing: 0.99% (363/36638)\n","Processing: 0.99% (364/36638)\n","Processing: 1.00% (365/36638)\n","Processing: 1.00% (366/36638)\n","Processing: 1.00% (367/36638)\n","Processing: 1.00% (368/36638)\n","Processing: 1.01% (369/36638)\n","Processing: 1.01% (370/36638)\n","Processing: 1.01% (371/36638)\n","Processing: 1.02% (372/36638)\n","Processing: 1.02% (373/36638)\n","Processing: 1.02% (374/36638)\n","Processing: 1.02% (375/36638)\n","Processing: 1.03% (376/36638)\n","Processing: 1.03% (377/36638)\n","Processing: 1.03% (378/36638)\n","Processing: 1.03% (379/36638)\n","Processing: 1.04% (380/36638)\n","Processing: 1.04% (381/36638)\n","Processing: 1.04% (382/36638)\n","Processing: 1.05% (383/36638)\n","Processing: 1.05% (384/36638)\n","Processing: 1.05% (385/36638)\n","Processing: 1.05% (386/36638)\n","Processing: 1.06% (387/36638)\n","Processing: 1.06% (388/36638)\n","Processing: 1.06% (389/36638)\n","Processing: 1.06% (390/36638)\n","Processing: 1.07% (391/36638)\n","Processing: 1.07% (392/36638)\n","Processing: 1.07% (393/36638)\n","Processing: 1.08% (394/36638)\n","Processing: 1.08% (395/36638)\n","Processing: 1.08% (396/36638)\n","Processing: 1.08% (397/36638)\n","Processing: 1.09% (398/36638)\n","Processing: 1.09% (399/36638)\n","Processing: 1.09% (400/36638)\n","Processing: 1.09% (401/36638)\n","Processing: 1.10% (402/36638)\n","Processing: 1.10% (403/36638)\n","Processing: 1.10% (404/36638)\n","Processing: 1.11% (405/36638)\n","Processing: 1.11% (406/36638)\n","Processing: 1.11% (407/36638)\n","Processing: 1.11% (408/36638)\n","Processing: 1.12% (409/36638)\n","Processing: 1.12% (410/36638)\n","Processing: 1.12% (411/36638)\n","Processing: 1.12% (412/36638)\n","Processing: 1.13% (413/36638)\n","Processing: 1.13% (414/36638)\n","Processing: 1.13% (415/36638)\n","Processing: 1.14% (416/36638)\n","Processing: 1.14% (417/36638)\n","Processing: 1.14% (418/36638)\n","Processing: 1.14% (419/36638)\n","Processing: 1.15% (420/36638)\n","Processing: 1.15% (421/36638)\n","Processing: 1.15% (422/36638)\n","Processing: 1.15% (423/36638)\n","Processing: 1.16% (424/36638)\n","Processing: 1.16% (425/36638)\n","Processing: 1.16% (426/36638)\n","Processing: 1.17% (427/36638)\n","Processing: 1.17% (428/36638)\n","Processing: 1.17% (429/36638)\n","Processing: 1.17% (430/36638)\n","Processing: 1.18% (431/36638)\n","Processing: 1.18% (432/36638)\n","Processing: 1.18% (433/36638)\n","Processing: 1.18% (434/36638)\n","Processing: 1.19% (435/36638)\n","Processing: 1.19% (436/36638)\n","Processing: 1.19% (437/36638)\n","Processing: 1.20% (438/36638)\n","Processing: 1.20% (439/36638)\n","Processing: 1.20% (440/36638)\n","Processing: 1.20% (441/36638)\n","Processing: 1.21% (442/36638)\n","Processing: 1.21% (443/36638)\n","Processing: 1.21% (444/36638)\n","Processing: 1.21% (445/36638)\n","Processing: 1.22% (446/36638)\n","Processing: 1.22% (447/36638)\n","Processing: 1.22% (448/36638)\n","Processing: 1.23% (449/36638)\n","Processing: 1.23% (450/36638)\n","Processing: 1.23% (451/36638)\n","Processing: 1.23% (452/36638)\n","Processing: 1.24% (453/36638)\n","Processing: 1.24% (454/36638)\n","Processing: 1.24% (455/36638)\n","Processing: 1.24% (456/36638)\n","Processing: 1.25% (457/36638)\n","Processing: 1.25% (458/36638)\n","Processing: 1.25% (459/36638)\n","Processing: 1.26% (460/36638)\n","Processing: 1.26% (461/36638)\n","Processing: 1.26% (462/36638)\n","Processing: 1.26% (463/36638)\n","Processing: 1.27% (464/36638)\n","Processing: 1.27% (465/36638)\n","Processing: 1.27% (466/36638)\n","Processing: 1.27% (467/36638)\n","Processing: 1.28% (468/36638)\n","Processing: 1.28% (469/36638)\n","Processing: 1.28% (470/36638)\n","Processing: 1.29% (471/36638)\n","Processing: 1.29% (472/36638)\n","Processing: 1.29% (473/36638)\n","Processing: 1.29% (474/36638)\n","Processing: 1.30% (475/36638)\n","Processing: 1.30% (476/36638)\n","Processing: 1.30% (477/36638)\n","Processing: 1.30% (478/36638)\n","Processing: 1.31% (479/36638)\n","Processing: 1.31% (480/36638)\n","Processing: 1.31% (481/36638)\n","Processing: 1.32% (482/36638)\n","Processing: 1.32% (483/36638)\n","Processing: 1.32% (484/36638)\n","Processing: 1.32% (485/36638)\n","Processing: 1.33% (486/36638)\n","Processing: 1.33% (487/36638)\n","Processing: 1.33% (488/36638)\n","Processing: 1.33% (489/36638)\n","Processing: 1.34% (490/36638)\n","Processing: 1.34% (491/36638)\n","Processing: 1.34% (492/36638)\n","Processing: 1.35% (493/36638)\n","Processing: 1.35% (494/36638)\n","Processing: 1.35% (495/36638)\n","Processing: 1.35% (496/36638)\n","Processing: 1.36% (497/36638)\n","Processing: 1.36% (498/36638)\n","Processing: 1.36% (499/36638)\n","Processing: 1.36% (500/36638)\n","Processing: 1.37% (501/36638)\n","Processing: 1.37% (502/36638)\n","Processing: 1.37% (503/36638)\n","Processing: 1.38% (504/36638)\n","Processing: 1.38% (505/36638)\n","Processing: 1.38% (506/36638)\n","Processing: 1.38% (507/36638)\n","Processing: 1.39% (508/36638)\n","Processing: 1.39% (509/36638)\n","Processing: 1.39% (510/36638)\n","Processing: 1.39% (511/36638)\n","Processing: 1.40% (512/36638)\n","Processing: 1.40% (513/36638)\n","Processing: 1.40% (514/36638)\n","Processing: 1.41% (515/36638)\n","Processing: 1.41% (516/36638)\n","Processing: 1.41% (517/36638)\n","Processing: 1.41% (518/36638)\n","Processing: 1.42% (519/36638)\n","Processing: 1.42% (520/36638)\n","Processing: 1.42% (521/36638)\n","Processing: 1.42% (522/36638)\n","Processing: 1.43% (523/36638)\n","Processing: 1.43% (524/36638)\n","Processing: 1.43% (525/36638)\n","Processing: 1.44% (526/36638)\n","Processing: 1.44% (527/36638)\n","Processing: 1.44% (528/36638)\n","Processing: 1.44% (529/36638)\n","Processing: 1.45% (530/36638)\n","Processing: 1.45% (531/36638)\n","Processing: 1.45% (532/36638)\n","Processing: 1.45% (533/36638)\n","Processing: 1.46% (534/36638)\n","Processing: 1.46% (535/36638)\n","Processing: 1.46% (536/36638)\n","Processing: 1.47% (537/36638)\n","Processing: 1.47% (538/36638)\n","Processing: 1.47% (539/36638)\n","Processing: 1.47% (540/36638)\n","Processing: 1.48% (541/36638)\n","Processing: 1.48% (542/36638)\n","Processing: 1.48% (543/36638)\n","Processing: 1.48% (544/36638)\n","Processing: 1.49% (545/36638)\n","Processing: 1.49% (546/36638)\n","Processing: 1.49% (547/36638)\n","Processing: 1.50% (548/36638)\n","Processing: 1.50% (549/36638)\n","Processing: 1.50% (550/36638)\n","Processing: 1.50% (551/36638)\n","Processing: 1.51% (552/36638)\n","Processing: 1.51% (553/36638)\n","Processing: 1.51% (554/36638)\n","Processing: 1.51% (555/36638)\n","Processing: 1.52% (556/36638)\n","Processing: 1.52% (557/36638)\n","Processing: 1.52% (558/36638)\n","Processing: 1.53% (559/36638)\n","Processing: 1.53% (560/36638)\n","Processing: 1.53% (561/36638)\n","Processing: 1.53% (562/36638)\n","Processing: 1.54% (563/36638)\n","Processing: 1.54% (564/36638)\n","Processing: 1.54% (565/36638)\n","Processing: 1.54% (566/36638)\n","Processing: 1.55% (567/36638)\n","Processing: 1.55% (568/36638)\n","Processing: 1.55% (569/36638)\n","Processing: 1.56% (570/36638)\n","Processing: 1.56% (571/36638)\n","Processing: 1.56% (572/36638)\n","Processing: 1.56% (573/36638)\n","Processing: 1.57% (574/36638)\n","Processing: 1.57% (575/36638)\n","Processing: 1.57% (576/36638)\n","Processing: 1.57% (577/36638)\n","Processing: 1.58% (578/36638)\n","Processing: 1.58% (579/36638)\n","Processing: 1.58% (580/36638)\n","Processing: 1.59% (581/36638)\n","Processing: 1.59% (582/36638)\n","Processing: 1.59% (583/36638)\n","Processing: 1.59% (584/36638)\n","Processing: 1.60% (585/36638)\n","Processing: 1.60% (586/36638)\n","Processing: 1.60% (587/36638)\n","Processing: 1.60% (588/36638)\n","Processing: 1.61% (589/36638)\n","Processing: 1.61% (590/36638)\n","Processing: 1.61% (591/36638)\n","Processing: 1.62% (592/36638)\n","Processing: 1.62% (593/36638)\n","Processing: 1.62% (594/36638)\n","Processing: 1.62% (595/36638)\n","Processing: 1.63% (596/36638)\n","Processing: 1.63% (597/36638)\n","Processing: 1.63% (598/36638)\n","Processing: 1.63% (599/36638)\n","Processing: 1.64% (600/36638)\n","Processing: 1.64% (601/36638)\n","Processing: 1.64% (602/36638)\n","Processing: 1.65% (603/36638)\n","Processing: 1.65% (604/36638)\n","Processing: 1.65% (605/36638)\n","Processing: 1.65% (606/36638)\n","Processing: 1.66% (607/36638)\n","Processing: 1.66% (608/36638)\n","Processing: 1.66% (609/36638)\n","Processing: 1.66% (610/36638)\n","Processing: 1.67% (611/36638)\n","Processing: 1.67% (612/36638)\n","Processing: 1.67% (613/36638)\n","Processing: 1.68% (614/36638)\n","Processing: 1.68% (615/36638)\n","Processing: 1.68% (616/36638)\n","Processing: 1.68% (617/36638)\n","Processing: 1.69% (618/36638)\n","Processing: 1.69% (619/36638)\n","Processing: 1.69% (620/36638)\n","Processing: 1.69% (621/36638)\n","Encoding error for text '['पौडेल/सानफ्रान्सिस्को']': index 19 is out of bounds for axis 0 with size 19\n","Processing: 1.70% (622/36638)\n","Processing: 1.70% (623/36638)\n","Processing: 1.70% (624/36638)\n","Processing: 1.71% (625/36638)\n","Processing: 1.71% (626/36638)\n","Processing: 1.71% (627/36638)\n","Processing: 1.71% (628/36638)\n","Processing: 1.72% (629/36638)\n","Processing: 1.72% (630/36638)\n","Processing: 1.72% (631/36638)\n","Processing: 1.72% (632/36638)\n","Processing: 1.73% (633/36638)\n","Processing: 1.73% (634/36638)\n","Processing: 1.73% (635/36638)\n","Processing: 1.74% (636/36638)\n","Processing: 1.74% (637/36638)\n","Processing: 1.74% (638/36638)\n","Processing: 1.74% (639/36638)\n","Processing: 1.75% (640/36638)\n","Processing: 1.75% (641/36638)\n","Processing: 1.75% (642/36638)\n","Processing: 1.76% (643/36638)\n","Processing: 1.76% (644/36638)\n","Processing: 1.76% (645/36638)\n","Processing: 1.76% (646/36638)\n","Processing: 1.77% (647/36638)\n","Processing: 1.77% (648/36638)\n","Processing: 1.77% (649/36638)\n","Processing: 1.77% (650/36638)\n","Processing: 1.78% (651/36638)\n","Processing: 1.78% (652/36638)\n","Processing: 1.78% (653/36638)\n","Processing: 1.79% (654/36638)\n","Processing: 1.79% (655/36638)\n","Processing: 1.79% (656/36638)\n","Processing: 1.79% (657/36638)\n","Processing: 1.80% (658/36638)\n","Processing: 1.80% (659/36638)\n","Processing: 1.80% (660/36638)\n","Processing: 1.80% (661/36638)\n","Processing: 1.81% (662/36638)\n","Processing: 1.81% (663/36638)\n","Processing: 1.81% (664/36638)\n","Processing: 1.82% (665/36638)\n","Processing: 1.82% (666/36638)\n","Processing: 1.82% (667/36638)\n","Processing: 1.82% (668/36638)\n","Processing: 1.83% (669/36638)\n","Processing: 1.83% (670/36638)\n","Processing: 1.83% (671/36638)\n","Processing: 1.83% (672/36638)\n","Processing: 1.84% (673/36638)\n","Processing: 1.84% (674/36638)\n","Processing: 1.84% (675/36638)\n","Processing: 1.85% (676/36638)\n","Processing: 1.85% (677/36638)\n","Processing: 1.85% (678/36638)\n","Processing: 1.85% (679/36638)\n","Processing: 1.86% (680/36638)\n","Processing: 1.86% (681/36638)\n","Processing: 1.86% (682/36638)\n","Processing: 1.86% (683/36638)\n","Processing: 1.87% (684/36638)\n","Processing: 1.87% (685/36638)\n","Processing: 1.87% (686/36638)\n","Processing: 1.88% (687/36638)\n","Processing: 1.88% (688/36638)\n","Processing: 1.88% (689/36638)\n","Processing: 1.88% (690/36638)\n","Processing: 1.89% (691/36638)\n","Processing: 1.89% (692/36638)\n","Processing: 1.89% (693/36638)\n","Processing: 1.89% (694/36638)\n","Processing: 1.90% (695/36638)\n"]}],"source":["def process_full_file(input_file, output_file):\n","    # Read the input file\n","    with open(input_file, 'r', encoding='utf-8') as file:\n","        text = file.read()\n","\n","    # Split text into sentences\n","    sentences = text.split('।')\n","\n","    # Process each sentence\n","    roman_translation = []\n","    total_sentences = len(sentences)\n","\n","    for i, sentence in enumerate(sentences, 1):\n","        # Skip empty sentences\n","        if not sentence.strip():\n","            continue\n","\n","        # Process words in the sentence\n","        words = sentence.split()\n","        translated_words = []\n","\n","        for word in words:\n","            # Handle numbers separately\n","            if any(char.isdigit() for char in word):\n","                translated_words.append(map_numbers(word))\n","                continue\n","\n","            # Clean and transliterate\n","            clean_text = clean(word)\n","            if not clean_text:\n","                continue\n","\n","            try:\n","                translated_word = transliterate(clean_text)\n","                translated_words.append(translated_word)\n","            except Exception as e:\n","                print(f\"Error transliterating word '{word}': {e}\")\n","                translated_words.append(word)\n","\n","        # Combine translated words\n","        if translated_words:\n","            translated_sentence = \" \".join(translated_words)\n","            # Capitalize first letter\n","            translated_sentence = translated_sentence[0].upper() + translated_sentence[1:]\n","            roman_translation.append(translated_sentence + '.')\n","\n","        # Print progress\n","        progress = (i / total_sentences) * 100\n","        print(f\"Processing: {progress:.2f}% ({i}/{total_sentences})\")\n","\n","    # Write to output file\n","    with open(output_file, 'w', encoding='utf-8') as file:\n","        file.write('\\n'.join(roman_translation))\n","\n","    print(f\"Transliteration complete. Output saved to {output_file}\")\n","\n","# Usage\n","input_text = \"./nepali_1.txt\"\n","output_text = \"./output_1.txt\"\n","process_full_file(input_text, output_text)"]},{"cell_type":"markdown","metadata":{"id":"pscwFkLp7aiH"},"source":["## optimize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OLlzkMU_7YwC"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPEkv2HkxnUQH+egoTalydw","gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}